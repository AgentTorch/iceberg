"""
This script computes the risk and opportunity for each state in the US. Part of v2

It uses the following files:
- national_summary_detailed.csv: Contains all jobs in the US and their economic value, number of employees, and industry type
- modified_bls_super_sector_df.csv: Maps OCC_CODE to industry type
- skills_based_risk.csv: Risk computed using heuristics discovered by LLM over WEF report
- opportunity_jobs_v2.csv: Contains the job roles that can be enhanced by AI (which in turn was generated by `find_opportunity_jobs.py`)

"""

import json
import pandas as pd
import numpy as np
from tqdm import tqdm
import os

project_root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

national_detailed_file_path = f'{project_root_path}/generation/national_summary_detailed.csv' # Contains all jobs in the US and their economic value, number of employees, and industry type
industry_type_path = f'{project_root_path}/v2_assets/modified_bls_super_sector_df.csv' # Maps OCC_CODE to industry type
wef_risk_path = f'{project_root_path}/v2_assets/skills_based_risk.csv' # Risk computed using heuristics discovered by LLM over WEF report
job_opportunity_path = f'{project_root_path}/v2_assets/opportunity_jobs_v2.csv' # Contains the job roles that can be enhanced by AI
data_v2_path = f'{project_root_path}/generation_v2'
state_results_csv_path = f'{project_root_path}/generation_v2/{{state}}_results.json'

AUTOMATION_RISK_PERCENTILE_THRESHOLD = 0.8 # all jobs with automation risk percentile above this threshold are considered at risk


#### Utilitiy functions ####

def get_state_file(state: str):
    state_sentence_case = f'{state[0].capitalize()}{state[1:]}'
    return state_sentence_case

def load_national_detailed_df(job_risk_df: pd.DataFrame):
    national_detailed_df = pd.read_csv(national_detailed_file_path)
    industry_type_df = pd.read_csv(industry_type_path)

    industry_type_df = industry_type_df.drop(columns=['Title'])
    industry_type_df['OCC_CODE'] = industry_type_df['O*NET-SOC Code'].apply(lambda x: x.split('.')[0] if '.' in x else x)
    industry_type_df = industry_type_df.drop(columns=['O*NET-SOC Code'])
    industry_type_df = industry_type_df.drop_duplicates(subset=['OCC_CODE'], keep='first')
    national_detailed_df = national_detailed_df.merge(industry_type_df, on='OCC_CODE', how='left')
    
    # Merge job risk df with national detailed df
    national_detailed_df = national_detailed_df.merge(
        job_risk_df[['OCC_CODE', 'automation_risk_score', 'perc_ile_thresholded_risk', 'adoption_rate']],
        on='OCC_CODE',
        how='left'  # or 'right' or 'inner' depending on your needs
    )
    national_detailed_df['automation_risk_score'] = national_detailed_df['automation_risk_score'].fillna(national_detailed_df['automation_risk_score'].median())
    
    # adoption_rate = hot_tech_ratio (0-1 scale)
    #   - Proxy for technology adoption speed in the real world
    #   - Hypothesis: Jobs using "hot tech" tools get automated faster
    #   - Based on current technology usage patterns
    #   - 0-1 scale

    # automation_risk_score (0-100 scale, normalized to 0-1)
    #   - Likelihood of job automation based on skill composition (basic_skills, cognitive_skills, etc.)
    #   - Higher scores = more routine/automatable work

    # Displacement_rate (0-1 scale)
    #   - Final probability of job displacement, mathematically the product of "automation feasibility" & "adoption status in real world"
    #   - Results in realistic partial displacement

    national_detailed_df['displacement_rate'] = national_detailed_df['adoption_rate'] * (national_detailed_df['automation_risk_score'] / 100.0)
    national_detailed_df['RISK_EMP'] = (national_detailed_df['TOT_EMP'] * national_detailed_df['displacement_rate']).fillna(0).astype(int)
    national_detailed_df['AT_RISK_ECONOMIC_VALUE'] = (national_detailed_df['economic_value'] * national_detailed_df['displacement_rate']).fillna(0).astype(int)
    
    # Set pandas option to avoid downcasting warning
    pd.set_option('future.no_silent_downcasting', True)
    national_detailed_df['perc_ile_thresholded_risk'] = national_detailed_df['perc_ile_thresholded_risk'].fillna(False)
    return national_detailed_df

def load_job_risk_df():
    job_risk_df = pd.read_csv(wef_risk_path)
    job_risk_df['OCC_CODE'] = job_risk_df['O*NET-SOC Code'].apply(lambda x: x.split('.')[0] if '.' in x else x)
    job_risk_df = job_risk_df.drop_duplicates(subset=['OCC_CODE'], keep='first') # as we have detailed jobs

    threshold = job_risk_df['automation_risk_score'].quantile(AUTOMATION_RISK_PERCENTILE_THRESHOLD)

    print(f"\033[91m Risk Score Threshold at percentile p{AUTOMATION_RISK_PERCENTILE_THRESHOLD*100}: {threshold}\033[0m")

    # Create the new field based on whether each row's score is above the threshold
    job_risk_df['perc_ile_thresholded_risk'] = job_risk_df['automation_risk_score'] >= threshold
    return job_risk_df

def load_opportunity_jobs_df():
    """
    Returns the opportunity jobs df v2
    """
    opportunity_df_v2 = pd.read_csv(job_opportunity_path)
    opportunity_df_v2 = opportunity_df_v2.rename(columns={'O*NET-SOC Code': 'OCC_CODE'})
    opportunity_df_v2['OCC_CODE'] = opportunity_df_v2['OCC_CODE'].apply(lambda x: x.split('.')[0])
    opportunity_df_v2 = opportunity_df_v2.drop_duplicates(subset=['OCC_CODE'], keep='first')  # Remove duplicate OCC_CODEs to avoid inflated employment numbers
    opportunity_df_v2 = opportunity_df_v2[opportunity_df_v2['AI_Transformation_Classification'] == 'High Transformation Potential']
    opportunity_df_v2 = opportunity_df_v2[(opportunity_df_v2['Tech_Stratum']=='Low-Tech') | (opportunity_df_v2['Tech_Stratum']=='Medium-Tech')]
    return opportunity_df_v2


### Risk calculations ####

def get_risk_industry_wise_impact_data(filtered_df: pd.DataFrame, risk_filtered_df: pd.DataFrame):
    # First, let's calculate the total TOT_EMP for each minor_group
    total_emp_by_industry = filtered_df.groupby(['Modified BLS Super Sector'])['TOT_EMP'].sum()
    # Then, calculate the TOT_EMP for jobs with perc_ile_thresholded_risk=True
    risk_emp_by_industry = risk_filtered_df.groupby(['Modified BLS Super Sector'])['RISK_EMP'].sum()
    # Calculate the percentage
    percentage_at_risk = (risk_emp_by_industry / total_emp_by_industry * 100).replace(np.nan, 0).sort_values(ascending=False)
    return percentage_at_risk.to_dict()

def get_risk_industry_wise_economic_value_data(risk_filtered_df: pd.DataFrame):
    risk_economic_value_by_industry = risk_filtered_df.groupby(['Modified BLS Super Sector'])['AT_RISK_ECONOMIC_VALUE'].sum()
    risk_economic_value_by_industry = risk_economic_value_by_industry.sort_values(ascending=False)
    return risk_economic_value_by_industry.to_dict()

def get_risk_industry_wise_employment_data(risk_filtered_df: pd.DataFrame):
    risk_emp_by_industry = risk_filtered_df.groupby(['Modified BLS Super Sector'])['RISK_EMP'].sum()
    risk_emp_by_industry = risk_emp_by_industry.sort_values(ascending=False)
    return risk_emp_by_industry.to_dict()

def get_employment_by_industry_data(filtered_df: pd.DataFrame):
    employment_by_industry = filtered_df.groupby(['Modified BLS Super Sector'])['TOT_EMP'].sum()
    employment_by_industry = employment_by_industry.sort_values(ascending=False)
    return employment_by_industry.to_dict()

def get_economic_value_by_industry_data(filtered_df: pd.DataFrame):
    economic_value_by_industry = filtered_df.groupby(['Modified BLS Super Sector'])['economic_value'].sum()
    economic_value_by_industry = economic_value_by_industry.sort_values(ascending=False)
    return economic_value_by_industry.to_dict()

def get_state_iceberg_index(filtered_df: pd.DataFrame, risk_filtered_df: pd.DataFrame)-> dict:
    """
    Calculate the state-wide Iceberg Index using economic value weighting.
    
    The state Iceberg Index is a weighted average of industry Iceberg Indices,
    where weights are based on each industry's proportion of total economic value.
    
    Args:
        filtered_df: Full DataFrame with all jobs for the state/region
        risk_filtered_df: DataFrame filtered to only jobs at risk
    
    Returns:
        dict: Contains state_iceberg_index and breakdown by industry
    """
    # Calculate total economic value for the state
    total_economic_value = filtered_df['economic_value'].sum()
    total_at_risk_economic_value = risk_filtered_df['AT_RISK_ECONOMIC_VALUE'].sum()
    
    if total_economic_value == 0:
        return {
            'state_iceberg_index': 0.0,
            'industry_breakdown': {},
            'total_economic_value': 0,
            'total_at_risk_economic_value': 0
        }
    
    # Calculate simple state-wide Iceberg Index
    state_iceberg_index = (total_at_risk_economic_value / total_economic_value) * 100
    
    # Get industry-wise breakdown for additional context
    industry_breakdown = {}
    
    # Get total economic value by industry from full dataset
    total_econ_by_industry = filtered_df.groupby('Modified BLS Super Sector')['economic_value'].sum()
    
    # Get at-risk economic value by industry from risk dataset
    risk_econ_by_industry = risk_filtered_df.groupby('Modified BLS Super Sector')['AT_RISK_ECONOMIC_VALUE'].sum()
    
    for industry in total_econ_by_industry.index:
        if pd.notna(industry):
            industry_economic_value = total_econ_by_industry[industry]
            industry_at_risk_value = risk_econ_by_industry.get(industry, 0)  # 0 if no risk jobs in this industry
            
            # Calculate industry's contribution to state Iceberg Index
            weight = industry_economic_value / total_economic_value
            industry_iceberg_index = (industry_at_risk_value / industry_economic_value * 100) if industry_economic_value > 0 else 0
            weighted_contribution = weight * industry_iceberg_index
            
            industry_breakdown[industry] = {
                'industry_iceberg_index': round(industry_iceberg_index, 2),
                'economic_value_weight': round(weight * 100, 2),  # as percentage
                'weighted_contribution': round(weighted_contribution, 2),
                'economic_value': int(industry_economic_value),
                'at_risk_economic_value': int(industry_at_risk_value)
            }
    
    # Sort industry breakdown by weighted contribution
    industry_breakdown = dict(sorted(industry_breakdown.items(), 
                                   key=lambda x: x[1]['weighted_contribution'], reverse=True))
    
    return {
        'state_iceberg_index': round(state_iceberg_index, 2),
        'industry_breakdown': industry_breakdown,
        'total_economic_value': int(total_economic_value),
        'total_at_risk_economic_value': int(total_at_risk_economic_value)
    }

def get_top_5_by_industry(df: pd.DataFrame, industry_type_column: str = 'Modified BLS Super Sector', dict_dump_mode: bool = False, is_risk: bool = False):
    """
    Get top 5 rows by TOT_EMP and economic_value for each industry group
    """
    results = {}

    EMP_COL = 'TOT_EMP' if not is_risk else 'RISK_EMP'
    ECON_COL = 'economic_value' if not is_risk else 'AT_RISK_ECONOMIC_VALUE'
    
    for industry in df[industry_type_column].unique():
        if pd.notna(industry):  # Skip NaN values
            industry_data = df[df[industry_type_column] == industry]

            if not dict_dump_mode:
                results[industry] = {
                    'top_5_by_employment': industry_data.nlargest(5, EMP_COL)[['OCC_CODE', 'OCC_TITLE', EMP_COL, ECON_COL]],
                    'top_5_by_economic_value': industry_data.nlargest(5, ECON_COL)[['OCC_CODE', 'OCC_TITLE', EMP_COL, ECON_COL]]
                }
            else:
                results[industry] = {
                    'top_5_by_employment': industry_data.nlargest(5, EMP_COL)[['OCC_CODE', 'OCC_TITLE', EMP_COL, ECON_COL]].to_dict(orient='records'),
                    'top_5_by_economic_value': industry_data.nlargest(5, ECON_COL)[['OCC_CODE', 'OCC_TITLE', EMP_COL, ECON_COL]].to_dict(orient='records')
                }
    
    return results

### Opportunity calculations ####

def get_opportunity_industry_wise_impact_data(filtered_df: pd.DataFrame, opportunity_mapped_filtered_df: pd.DataFrame):
    total_emp_by_industry = filtered_df.groupby(['Modified BLS Super Sector'])['TOT_EMP'].sum()
    opportunity_emp_by_industry = opportunity_mapped_filtered_df.groupby(['Modified BLS Super Sector'])['TOT_EMP'].sum()
    percentage_opportunity = (opportunity_emp_by_industry / total_emp_by_industry * 100).replace(np.nan, 0).sort_values(ascending=False)
    return percentage_opportunity.to_dict()

def get_opportunity_industry_wise_economic_value_data(opportunity_mapped_filtered_df: pd.DataFrame):
    opportunity_economic_value_by_industry = opportunity_mapped_filtered_df.groupby(['Modified BLS Super Sector'])['economic_value'].sum()
    opportunity_economic_value_by_industry = opportunity_economic_value_by_industry.sort_values(ascending=False)
    return opportunity_economic_value_by_industry.to_dict()

def get_opportunity_industry_wise_employment_data(opportunity_mapped_filtered_df: pd.DataFrame):
    opportunity_emp_by_industry = opportunity_mapped_filtered_df.groupby(['Modified BLS Super Sector'])['TOT_EMP'].sum()
    opportunity_emp_by_industry = opportunity_emp_by_industry.sort_values(ascending=False)
    return opportunity_emp_by_industry.to_dict()

################################################################################

def compute_risk_for_state(national_detailed_df: pd.DataFrame, state_title: str, opportunity_df_v2: pd.DataFrame):
    '''
        Computes the risk for a given state.
        Args:
            national_detailed_df: pd.DataFrame
            state_title: str of form  'North Carolina', if its None, then it will compute the risk for all states
    '''
    filtered_df = national_detailed_df
    if state_title:
        filtered_df = national_detailed_df[national_detailed_df['AREA_TITLE'] == state_title]

    # ----------------------------- Risk -----------------------------
    risk_filtered_df = filtered_df[filtered_df['perc_ile_thresholded_risk']]

    # ----------------------------- Opportunity -----------------------------
    opportunity_mapped_filtered_df = opportunity_df_v2.merge(filtered_df, on='OCC_CODE', how='inner')
    

    
    data = {
        "state": state_title if state_title else "National",
        "total_jobs": filtered_df['TOT_EMP'].sum(),
        "total_economic_value": filtered_df['economic_value'].sum(),
        "employment_by_industry": get_employment_by_industry_data(filtered_df),
        "economic_value_by_industry": get_economic_value_by_industry_data(filtered_df),
        "state_iceberg_index": get_state_iceberg_index(filtered_df, risk_filtered_df),

        "risk": {
            "total_jobs_at_risk": risk_filtered_df['TOT_EMP'].sum(),
            "total_jobs_at_risk_percentage": (risk_filtered_df['TOT_EMP'].sum() / filtered_df['TOT_EMP'].sum()) * 100,
            
            "total_economic_value_at_risk": risk_filtered_df['economic_value'].sum(),
            "total_economic_value_at_risk_percentage": (risk_filtered_df['economic_value'].sum() / filtered_df['economic_value'].sum()) * 100,

            "industry_impact_percentage": get_risk_industry_wise_impact_data(filtered_df, risk_filtered_df),
            "employment_at_risk_by_industry": get_risk_industry_wise_employment_data(risk_filtered_df),
            "economic_value_at_risk_by_industry": get_risk_industry_wise_economic_value_data(risk_filtered_df),
            "industry_wise_top_5_jobs": get_top_5_by_industry(risk_filtered_df, dict_dump_mode=True, is_risk=True)
        },
        "opportunity": {
            "total_jobs_opportunity": opportunity_mapped_filtered_df['TOT_EMP'].sum(),
            "total_jobs_opportunity_percentage": (opportunity_mapped_filtered_df['TOT_EMP'].sum() / filtered_df['TOT_EMP'].sum()) * 100,

            "total_economic_value_opportunity": opportunity_mapped_filtered_df['economic_value'].sum(),
            "total_economic_value_opportunity_percentage": (opportunity_mapped_filtered_df['economic_value'].sum() / filtered_df['economic_value'].sum()) * 100,

            "industry_impact_percentage": get_opportunity_industry_wise_impact_data(filtered_df, opportunity_mapped_filtered_df),
            "employment_opportunity_by_industry": get_opportunity_industry_wise_employment_data(opportunity_mapped_filtered_df),
            "economic_value_opportunity_by_industry": get_opportunity_industry_wise_economic_value_data(opportunity_mapped_filtered_df),
            "industry_wise_top_5_jobs": get_top_5_by_industry(opportunity_mapped_filtered_df, dict_dump_mode=True, is_risk=False)
        }
    }
    
    return data

def dump_data_to_csv(data: dict):
    state_title: str = data['state']
    state_title = state_title.lower().replace(' ', '_')
    path = state_results_csv_path.format(state=state_title)
    
    with open(path, 'w') as f:
        json.dump(data, f, indent=4)
    



def main():
    job_risk_df = load_job_risk_df()
    opportunity_df_v2 = load_opportunity_jobs_df()
    national_detailed_df = load_national_detailed_df(job_risk_df)
    
    
    na_counts = national_detailed_df.isna().sum()
    non_zero_counts = na_counts[na_counts > 0]
    print("Rows with missing values in columns:")
    for col, count in non_zero_counts.items():
        print(f"{col}: {count}")
    print(f'Total rows: {national_detailed_df.shape[0]}')
    print(f'-'*30)
    
    state_titles = national_detailed_df['AREA_TITLE'].unique().tolist()
    state_titles += [None]
    for state_title in tqdm(state_titles):
        data = compute_risk_for_state(national_detailed_df, state_title, opportunity_df_v2)
        dump_data_to_csv(data)

    

if __name__ == '__main__':
    main()